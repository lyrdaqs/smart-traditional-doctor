{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgpH_iZsy2Bv"
      },
      "source": [
        "https://dumps.wikimedia.org/fawiki/20230601/\n",
        "\n",
        "https://github.com/CyberZHG/wiki-dump-reader\n",
        "\n",
        "https://github.com/Ramaseshanr/IITMDS/blob/main/NLP_Workshop_IITMDS.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhfO_W62uXG1",
        "outputId": "f9260563-175e-475f-a4dd-11aa05ac4dee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "elasticsearch-oss-7.9.2-linux-x86_64.tar.gz: OK\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n",
        "wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512\n",
        "tar -xzf elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n",
        "sudo chown -R daemon:daemon elasticsearch-7.15.1/\n",
        "shasum -a 512 -c elasticsearch-oss-7.15.1-linux-x86_64.tar.gz.sha512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4kGFWkav3Qr"
      },
      "outputs": [],
      "source": [
        "%%bash --bg\n",
        "\n",
        "sudo -H -u daemon elasticsearch-7.9.2/bin/elasticsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9eH4uL0v8RR"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "curl -sX GET \"localhost:9200/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL435FbQx3dH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31e705e1-eb80-40b1-a8cc-bc5590f7d41a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://dumps.wikimedia.org/fawiki/20230601/fawiki-20230601-pages-articles.xml.bz2\n",
            "To: /content/fawiki-20230601-pages-articles.xml.bz2\n",
            "100% 1.05G/1.05G [03:37<00:00, 4.84MB/s]\n"
          ]
        }
      ],
      "source": [
        "# !gdown https://dumps.wikimedia.org/fawiki/20230601/fawiki-20230601-pages-articles-multistream.xml.bz2\n",
        "!gdown https://dumps.wikimedia.org/fawiki/20230601/fawiki-20230601-pages-articles.xml.bz2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf9X82cE_tfS"
      },
      "outputs": [],
      "source": [
        "!bunzip2 fawiki-20230601-pages-articles.xml.bz2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTZkRRZ7wfy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62fabd91-fda3-4ca0-8561-7436fbbf3001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wiki-dump-reader\n",
            "  Downloading wiki-dump-reader-0.0.4.tar.gz (3.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wiki-dump-reader\n",
            "  Building wheel for wiki-dump-reader (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wiki-dump-reader: filename=wiki_dump_reader-0.0.4-py3-none-any.whl size=3983 sha256=036c047366932786075119c0d7e7c1d5fdf29bf20846c0769ca45bb7c9880f1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/81/3d/463b7f906f65d3e9e43db8446ebc5fb719bf1777a40b411cd2\n",
            "Successfully built wiki-dump-reader\n",
            "Installing collected packages: wiki-dump-reader\n",
            "Successfully installed wiki-dump-reader-0.0.4\n",
            "Collecting elasticsearch\n",
            "  Downloading elasticsearch-8.9.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.5/395.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting elastic-transport<9,>=8 (from elasticsearch)\n",
            "  Downloading elastic_transport-8.4.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<2,>=1.26.2 (from elastic-transport<9,>=8->elasticsearch)\n",
            "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8->elasticsearch) (2023.7.22)\n",
            "Installing collected packages: urllib3, elastic-transport, elasticsearch\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.4\n",
            "    Uninstalling urllib3-2.0.4:\n",
            "      Successfully uninstalled urllib3-2.0.4\n",
            "Successfully installed elastic-transport-8.4.0 elasticsearch-8.9.0 urllib3-1.26.16\n"
          ]
        }
      ],
      "source": [
        "!pip install wiki-dump-reader\n",
        "!pip install elasticsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6M4ouV5DyEBs"
      },
      "outputs": [],
      "source": [
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "cloud_id = \"My_deployment:dXMtY2VudHJhbDEuZ2NwLmNsb3VkLmVzLmlvOjQ0MyRiNTk1ZDliNGM4MjA0NzA2YmU0NDVmNWQ4MDc4YWE0YiRkNGZlMmFjNzJkMWU0NWY1YjRkMmZiODNlNTU4NmVjNg==\"\n",
        "\n",
        "# Create Elasticsearch client instance\n",
        "client = Elasticsearch(cloud_id=cloud_id, basic_auth=('elastic', 'ujS3qg86qB7fy5WuqqlpX3CL'))\n",
        "\n",
        "# Now you can perform actions using the client\n",
        "response = client.search(index=\"your-index\", body={\"query\": {\"match_all\": {}}})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vymv_YOo1Cm_",
        "outputId": "b0fc273a-e05c-4f5e-cc66-1b967c72e953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-f880b89863d3>:16: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
            "  response = client.indices.create(index=index_name, body=settings)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'health_items'}\n"
          ]
        }
      ],
      "source": [
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "# Create an instance of Elasticsearch client\n",
        "es = Elasticsearch(cloud_id=cloud_id, basic_auth=('lyrdaq', '37771234'))\n",
        "\n",
        "# Define the index settings and mappings\n",
        "index_name = \"wiki\"\n",
        "settings = {\n",
        "    \"settings\": {\n",
        "        \"number_of_shards\": 5,\n",
        "        \"number_of_replicas\": 0\n",
        "    },\n",
        "    \"mappings\": {\n",
        "        \"properties\": {\n",
        "            \"title\": {\"type\": \"text\"},\n",
        "            \"text\": {\"type\": \"text\"},\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create the index with the specified settings and mappings\n",
        "response = es.indices.create(index=index_name, body=settings)\n",
        "\n",
        "# Print the response\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLfZZi0fwi6m"
      },
      "outputs": [],
      "source": [
        "from wiki_dump_reader import Cleaner,iterate\n",
        "\n",
        "cleaner = Cleaner()\n",
        "\n",
        "file_name='fawiki-20230601-pages-articles.xml'\n",
        "page_count = 0\n",
        "\n",
        "with open(f'{file_name}.cleaned.txt', 'w', encoding='utf-8') as output:\n",
        "  for title, text in iterate(file_name):\n",
        "    text = cleaner.clean_text(text)\n",
        "    cleaned_text, links = cleaner.build_links(text)\n",
        "    #print(f\"{title} -> {cleaned_text}\")\n",
        "\n",
        "    output.write(title + '\\n' + cleaned_text + '\\n')\n",
        "\n",
        "    page_count += 1\n",
        "    if page_count % 50 == 0:\n",
        "        print('Pages dumped = ', page_count)\n",
        "    # For demo purposes, the execution is stopped after 5 pages\n",
        "    # if page_count > 100:\n",
        "    #     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p_h_eFqVE87"
      },
      "outputs": [],
      "source": [
        "!pip install elasticsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBPksU-NLBjE"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Set up Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"WikipediaXMLParsing\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "OWdNxgNSmRE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from wiki_dump_reader import Cleaner, iterate\n",
        "\n",
        "spark = SparkSession.builder.appName(\"WikipediaXMLParsing\").getOrCreate()\n",
        "\n",
        "file_name = 'fawiki-20230601-pages-articles.xml'\n",
        "cleaner = Cleaner()\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "@udf(StringType())\n",
        "def clean_text(text):\n",
        "    return cleaner.clean_text(text)\n",
        "\n",
        "\n",
        "wikipedia_df = spark.read.format(\"xml\") \\\n",
        "    .option(\"rowTag\", \"page\") \\\n",
        "    .load(file_name) \\\n",
        "    .withColumn(\"cleaned_text\", clean_text(col(\"revision.text._VALUE\"))) \\\n",
        "    .select(\"title\", \"cleaned_text\")\n",
        "\n",
        "\n",
        "wikipedia_df.show()\n",
        "\n",
        "wikipedia_df.write \\\n",
        "    .format(\"org.elasticsearch.spark.sql\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .option(\"es.resource\", \"wiki/doc\") \\\n",
        "    .option(\"es.nodes\", \"localhost\") \\\n",
        "    .option(\"es.port\", \"9200\") \\\n",
        "    .save()\n"
      ],
      "metadata": {
        "id": "UaJepNFUm30D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sUDnQ75Wqbb"
      },
      "outputs": [],
      "source": [
        "%reset -f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G-v2soiNGHV"
      },
      "outputs": [],
      "source": [
        "from wiki_dump_reader import Cleaner,iterate\n",
        "from elasticsearch.helpers import bulk\n",
        "\n",
        "cleaner = Cleaner()\n",
        "\n",
        "file_name='fawiki-20230601-pages-articles.xml'\n",
        "page_count = 0\n",
        "\n",
        "batch_data = []\n",
        "\n",
        "for title, text in iterate(file_name):\n",
        "  text = cleaner.clean_text(text)\n",
        "  cleaned_text, links = cleaner.build_links(text)\n",
        "  #print(f\"{title} -> {cleaned_text}\")\n",
        "  batch_data.append({\"_index\": \"wiki\", \"_source\":{\"title\": title, \"text\": cleaned_text}})\n",
        "  page_count += 1\n",
        "  if page_count % 100000 == 0:\n",
        "      bulk(es, batch_data)\n",
        "      batch_data = []\n",
        "      print('Pages dumped = ', page_count)\n",
        "\n",
        "bulk(es, batch_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JOVqxjRlfj0"
      },
      "outputs": [],
      "source": [
        "file_path = \"illness.txt\"\n",
        "\n",
        "with open(file_path, \"r\") as file:\n",
        "    illness = [line.strip() for line in file]\n",
        "\n",
        "file_path = \"medicines.txt\"\n",
        "\n",
        "with open(file_path, \"r\") as file:\n",
        "    medicines = [line.strip() for line in file]\n",
        "\n",
        "file_path = \"signs.txt\"\n",
        "\n",
        "with open(file_path, \"r\") as file:\n",
        "    signs = [line.strip() for line in file]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s-_h7pIoorO"
      },
      "outputs": [],
      "source": [
        "medicines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXH8LfVwTG74"
      },
      "outputs": [],
      "source": [
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "cloud_id = \"wikipedia:dXMtY2VudHJhbDEuZ2NwLmNsb3VkLmVzLmlvOjQ0MyRjZGI0ZjYwMjI1ZTU0NjgyOGRiZDBlNWJhZmUxYWY1OSRlNzJhNjg4ZDU3MjI0YjM3OGVmZDlkYTFlM2M2MWFhMw==\"\n",
        "\n",
        "# Create Elasticsearch client instance\n",
        "client = Elasticsearch(cloud_id=cloud_id, basic_auth=('lyrdaq', '37771234'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "23huTLKVgKjm"
      },
      "outputs": [],
      "source": [
        "with open('graph2.txt', 'w', encoding='utf-8') as output:\n",
        "  for illnes in illness:\n",
        "    for sign in signs:\n",
        "      query = {\n",
        "        \"query\": {\n",
        "          \"bool\": {\n",
        "            \"must\": [\n",
        "              {\n",
        "                \"term\": {\n",
        "                  \"text\": illnes\n",
        "                }\n",
        "              },\n",
        "              {\n",
        "                \"term\": {\n",
        "                  \"text\": sign\n",
        "                }\n",
        "              }\n",
        "            ]\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      result = client.count(index=\"wiki\", body=query)\n",
        "      count = str(result[\"count\"])\n",
        "      output.write(illnes + \",\" + sign + \",\" + count + '\\n')\n",
        "      print(illnes, sign, count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9mYbKml3GsW",
        "outputId": "0231df02-e643-4b06-e979-86f4eb5cb9a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     medicine  sign  count\n",
            "9782     شوید   درد     75\n",
            "9801     شوید   ضعف     67\n",
            "3867     پیاز   درد     58\n",
            "8599     سبزی   درد     39\n",
            "3858     پیاز  تورم     33\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the text file as a DataFrame\n",
        "df = pd.read_csv('graph.txt', sep=',', header=None)\n",
        "\n",
        "# Rename the columns\n",
        "df.columns = ['medicine', 'sign', 'count']\n",
        "\n",
        "# Sort the DataFrame by the \"Number\" column in descending order\n",
        "df_sorted = df.sort_values(by='count', ascending=False)\n",
        "\n",
        "# Display the sorted DataFrame\n",
        "print(df_sorted.head())\n",
        "df_sorted.to_csv('graph_signs_medicines.csv', index=False, encoding='utf-8-sig')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('graph_medicine_ilness.csv')\n",
        "df.head()\n",
        "df.to_csv(\"graph_signs_medicines2.csv\", encoding='utf-8', index=False)"
      ],
      "metadata": {
        "id": "Ten7pUORZ0Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_input = '''سلام وقت بخیر.حدود ۳ ماه پیش کرونای خفیف گرفتم ولی تو اون مدت خیلی استرس داشتم بعد از پایان قرنطینه یک روز که بیرون بودم بعد از برگشت به خانه لرزش شدید پا گرفتم به همراه گیجی سر و الان حدود چند ماهی هست که حالت گیج و منگی بهمراه تپش قلب دارم به چند پزشک مراجعه کردم به اتفاق گفتند که دچار اضطراب شدم چند روزی هم دست و پاهام و سرم حالت گز گز و بی حسی پیدا کرده البته فقط چند روز هست که از قرص آرامبخش استفاده میکنم چون حس خوبی به قرص نداشتم ممنون میشم که نظرتون رو بگید'''\n",
        "\n",
        "def extract_words(text_input, word_list):\n",
        "    extracted_words = []\n",
        "    words = text_input.split()\n",
        "\n",
        "    for word in words:\n",
        "        if word in word_list:\n",
        "            extracted_words.append(word)\n",
        "\n",
        "    return extracted_words\n",
        "\n",
        "med = extract_words(text_input, illness)\n",
        "print(med)"
      ],
      "metadata": {
        "id": "CsqrNwDPg0IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud6BMlMKkOYT",
        "outputId": "58e3ff44-3370-498f-b4f8-83af7b21bc45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hazm in /usr/local/lib/python3.10/dist-packages (0.9.3)\n",
            "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.2)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.25.2)\n",
            "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.9)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.2.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (2.11.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (67.7.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (6.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import *\n",
        "\n",
        "stemmer = Stemmer()\n",
        "stemmer.stem(\"تپش\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y1WI0QcKnMBe",
        "outputId": "73cba69f-878c-4525-f424-a9797be15b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'تپ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import *\n",
        "\n",
        "text_input = '''سلام وقت بخیر.حدود ۳ ماه پیش کرونای خفیف گرفتم ولی تو اون مدت خیلی استرس داشتم بعد از پایان قرنطینه یک روز که بیرون بودم بعد از برگشت به خانه لرزش شدید پا گرفتم به همراه گیجی سر و الان حدود چند ماهی هست که حالت گیج و منگی بهمراه تپش قلب دارم به چند پزشک مراجعه کردم به اتفاق گفتند که دچار اضطراب شدم چند روزی هم دست و پاهام و سرم حالت گز گز و بی حسی پیدا کرده البته فقط چند روز هست که از قرص آرامبخش استفاده میکنم چون حس خوبی به قرص نداشتم ممنون میشم که نظرتون رو بگید'''\n",
        "\n",
        "def extract_words(text_input, word_list):\n",
        "    extracted_words = []\n",
        "    stemmer = Stemmer()\n",
        "    words = [stemmer.stem(word) for word in word_tokenize(text_input)]\n",
        "\n",
        "    for word in words:\n",
        "        if word in word_list:\n",
        "            extracted_words.append(word)\n",
        "\n",
        "    return extracted_words\n",
        "\n",
        "med = extract_words(text_input, signs)\n",
        "print(med)"
      ],
      "metadata": {
        "id": "L_39ufJPmrU5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "5099d213-c02e-4025-f6ed-ac279f7d4cf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9952cfa9b9c8>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mextracted_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'signs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"illness.txt\"\n",
        "\n",
        "with open(file_path, \"r\") as file:\n",
        "    illness = [line.strip() for line in file]\n",
        "\n",
        "file_path = \"medicines.txt\"\n",
        "\n",
        "with open(file_path, \"r\") as file:\n",
        "    medicines = [line.strip() for line in file]\n",
        "\n",
        "file_path = \"signs.txt\"\n",
        "\n",
        "with open(file_path, \"r\") as file:\n",
        "    signs = [line.strip() for line in file]"
      ],
      "metadata": {
        "id": "CBW1HZkKY6_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import *\n",
        "\n",
        "text_input=\"سردرد\"\n",
        "stemmer = Stemmer()\n",
        "\n",
        "def extract_ngrams(text_input, n):\n",
        "    ngrams = []\n",
        "    words = word_tokenize(text_input)\n",
        "    if len(words) >= n:\n",
        "        for i in range(len(words) - n + 1):\n",
        "            ngram = tuple(words[i:i+n])\n",
        "            ngrams.append(ngram)\n",
        "    return ngrams\n",
        "\n",
        "\n",
        "def filter_phrases(text_input, phrase_list, n):\n",
        "    matched_phrases = []\n",
        "    ngrams = extract_ngrams(text_input, n)\n",
        "\n",
        "    for ngram in ngrams:\n",
        "        phrase = \" \".join(ngram)\n",
        "        stem_phrase = \" \".join([stemmer.stem(gram) for gram in ngram])\n",
        "        phrase_ws = \"\".join(ngram)\n",
        "        stem_phrase_ws = \"\".join([stemmer.stem(gram) for gram in ngram])\n",
        "\n",
        "        if stem_phrase in phrase_list:\n",
        "            matched_phrases.append(stem_phrase)\n",
        "        if phrase in phrase_list:\n",
        "            matched_phrases.append(phrase)\n",
        "\n",
        "        if stem_phrase_ws in phrase_list:\n",
        "            matched_phrases.append(stem_phrase_ws)\n",
        "        if phrase_ws in phrase_list:\n",
        "            matched_phrases.append(phrase_ws)\n",
        "\n",
        "        for gram in ngram:\n",
        "            if gram in phrase_list:\n",
        "                matched_phrases.append(gram)\n",
        "\n",
        "            stem_gram = stemmer.stem(gram)\n",
        "            if stem_gram in phrase_list:\n",
        "                matched_phrases.append(stem_gram)\n",
        "\n",
        "    matched_phrases = list(set(matched_phrases))\n",
        "    return matched_phrases\n",
        "\n",
        "matched_signs = filter_phrases(text_input, signs, 2)\n",
        "matched_medicines = filter_phrases(text_input, medicines, 2)\n",
        "matched_illness = filter_phrases(text_input, illness, 2)\n",
        "\n",
        "matched_phrases = {\"matched_signs\": matched_signs,\n",
        "                   \"matched_medicines\": matched_medicines,\n",
        "                   \"matched_illness\":matched_illness}\n",
        "matched_phrases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVz0RucfTE9V",
        "outputId": "aa474355-98aa-4a88-c062-ee316276bbfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'matched_signs': [], 'matched_medicines': [], 'matched_illness': []}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}